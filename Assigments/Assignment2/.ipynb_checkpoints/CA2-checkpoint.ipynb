{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CA2 - Supervised machine learning classification pipeline - applied to medical data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part I: Data loading and data exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import necessary libraries/modules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'mlxtend'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/qf/9lcflp0s1nq8l85_tw03dz180000gp/T/ipykernel_88777/1626857627.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mmlxtend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassifier\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPerceptron\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAdaline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mseaborn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'mlxtend'"
     ]
    }
   ],
   "source": [
    "# Insert your code below\n",
    "# ======================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mlxtend.classifier import Perceptron, Adaline, LogisticRegression\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading and exploring data\n",
    "\n",
    "1. Load the dataset `fetal_health.csv` with `pandas`. Use the first column as the row index.\n",
    "2. Check for missing data, report on your finding and remove samples with missing data, if you find any.\n",
    "3. Display the raw data with appropriate plots/outputs and inspect it. Describe the distributions of the values of feature `\"baseline value\"`, `\"accelerations\"`, and the target variable `\"fetal_health\"`.\n",
    "4. Will it be beneficial to scale the data? Why or why not?\n",
    "5. Is the data linearly separable using a combination of any two pairs of features? Can we expect an accuracy close to 100% from a linear classifier?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert your code below\n",
    "# ======================\n",
    "# 1 \n",
    "df = pd.read_csv('assets/fetal_health.csv', index_col= 0)\n",
    "df.head() # view first five rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2\n",
    "df_NaN = df.isna() # rows containing missing data\n",
    "print(df_NaN.sum()) # view missing data -> there is None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3\n",
    "plt.figure(figsize = (5,6)) # alter the size, to makes the output look nicer\n",
    "\n",
    "titels = list(df.columns) #makes a loist with the headers from the dataframe\n",
    "for idx, colname in enumerate(titels):\n",
    "    plt.hist(df[colname], bins=20, color=\"skyblue\", edgecolor=\"white\") # changed color and added edgecolo to make it nicer\n",
    "    plt.title(f'{colname} Distribution', fontsize = 10) # Titel\n",
    "    plt.xlabel(f'{colname} Values')# x-axis title w\n",
    "    plt.ylabel('Frequency') # y-axis title\n",
    "    plt.show()  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.  -baseline value : describes the fetal heart rate in beats per minute. it is distrubuted almost as a normal distribution.\n",
    "    - acceleration: number of acceleration per minute, the distrubution is skewed right and the majority obseerved where 0.0.\n",
    "    - fetal health : the fetal health categorized into 0(normal) og 1(not normal). Most of them are 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "4. Will it be beneficial to scale the data? Why or why not?\n",
    "    - Yes, it would be beneficical to scale the data. this ois beacuse it makes the ontra variance smaller and for example the baseline, to nomalize this, the extreme values could be -2 and 2, versus 110 and 160. The calculations of Wx +b will be much easier then."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "5. Is the data linearly separable using a combination of any two pairs of features? Can we expect an accuracy close to 100% from a linear classifier?\n",
    "    - it is not perfect linearly separable. Looking at the plot below, you can see that its hard to draw a line that separates the classes completaly, therefor we can not expect an accuancy close to 100% from linear classifier.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5 \n",
    "sns.pairplot(data=df, hue = 'fetal_health')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part II: Train/Test Split\n",
    "\n",
    "Divide your dataset into training and testing subsets. Follow these steps to create the split:\n",
    "\n",
    "1. **Divide the dataset into two data sets, each data set only contains samples of either class 0 or class 1:**\n",
    "- Create a DataFrame `df_0` containing all data with `\"fetal_health\"` equal to 0.\n",
    "- Create a DataFrame `df_1` containing all data with `\"fetal_health\"` equal to 1.\n",
    "\n",
    "2. **Split into training and test set by randomly sampling entries from the data frames:**\n",
    "- Create a DataFrame `df_0_train` containing by sampling `75%` of the entries from `df_0` (use the `sample` method of the data frame, fix the `random_state` to `42`).\n",
    "- Create a DataFrame `df_1_train` using the same approach with `df_1`.\n",
    "- Create a DataFrame `df_0_test` containing the remaining entries of `df_0` (use `df_0.drop(df_0_train.index)` to drop all entries except the previously extracted ones).\n",
    "- Create a DataFrame `df_1_test` using the same approach with `df_1`.\n",
    "\n",
    "3. **Merge the datasets split by classes back together:**\n",
    "- Create a DataFrame `df_train` containing all entries from `df_0_train` and `df_1_train`. (Hint: use the `concat` method you know from CA1)\n",
    "- Create a DataFrame `df_test` containing all entries from the two test sets.\n",
    "\n",
    "4. **Create the following data frames from these splits:**\n",
    "- `X_train`: Contains all columns of `df_train` except for the target feature `\"fetal_health\"`\n",
    "- `X_test`: Contains all columns of `df_test` except for the target feature `\"fetal_health\"`\n",
    "- `y_train`: Contains only the target feature `\"fetal_health\"` for all samples in the training set\n",
    "- `y_test`: Contains only the target feature `\"fetal_health\"` for all samples in the test set\n",
    "\n",
    "5. **Check that your sets have the expected sizes/shape by printing number of rows and colums (\"shape\") of the data sets.**\n",
    "- (Sanity check: there should be 8 features, almost 1000 samples in the training set and slightly more than 300 samples in the test set.)\n",
    "\n",
    "\n",
    "6. **Explain the purpose of this slightly complicated procedure. Why did we first split into the two classes? Why did we then split into a training and a testing set?**\n",
    "\n",
    "\n",
    "7. **What is the share (in percent) of samples with class 0 label in test and training set, and in the intial data set?**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert your code below\n",
    "# ======================\n",
    "\n",
    "# 1\n",
    "df_0 = df[df['fetal_health'] == 0]\n",
    "df_1 = df[df['fetal_health'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 Split into training and test set by randomly sampling entries from the data frames:\n",
    "df_0_train = df_0.sample(frac = 0.75, random_state = 42)\n",
    "df_1_train = df_1.sample(frac = 0.75, random_state = 42)\n",
    "\n",
    "df_0_test = df_0.sample(frac = 0.75, random_state = 42)\n",
    "df_1_test = df_1.sample(frac = 0.75, random_state = 42)\n",
    "\n",
    "df_0_test = df_0.drop(df_0_train.index)\n",
    "df_1_test = df_1.drop(df_1_train.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Merge the datasets split by classes back together:\n",
    "df_train = pd.concat([df_0_train, df_1_train])\n",
    "df_test = pd.concat([df_0_test, df_1_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4 \n",
    "\n",
    "X_train = df_train.drop(columns=[\"fetal_health\"])\n",
    "X_test = df_test.drop(columns=[\"fetal_health\"])\n",
    "y_train = df_train['fetal_health']\n",
    "y_test = df_test['fetal_health']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#5  \n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. The purpose of first splitt into two classes is to undertand the patterns in the data and train the model on a portion of the data, and evaluate the preformance on unseen data to check and assess the capasity to generalize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7\n",
    "\n",
    "train_0_share = (y_train == 0).mean() * 100\n",
    "test_0_share = (y_test == 0).mean() * 100\n",
    "initial_0_share = (df['fetal_health'] == 0).mean() * 100\n",
    "\n",
    "\n",
    "print(\"{:.1f}%\".format(train_0_share))\n",
    "print(\"{:.1f}%\".format(test_0_share))\n",
    "print(\"{:.1f}%\".format(initial_0_share))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert data to numpy arrays and shuffle the training data\n",
    "\n",
    "Many machine learning models (including those you will work with later in the assignment) will not accept DataFrames as input. Instead, they will only work if you pass numpy arrays containing the data.\n",
    "Here, we convert the DataFrames `X_train`, `X_test`, `y_train`, and `y_test` to numpy arrays `X_train`, `X_test`, `y_train`, and `y_test`.\n",
    "\n",
    "Moreover we shuffle the training data. This is important because the training data is currently ordered by class. In Part IV, we use the first n samples from the training set to train the classifiers. If we did not shuffle the data, the classifiers would only be trained on samples of class 0.\n",
    "\n",
    "Nothing to be done here, just execute the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to numpy arrays\n",
    "X_train = X_train.to_numpy()\n",
    "X_test = X_test.to_numpy()\n",
    "y_train = y_train.to_numpy()\n",
    "y_test = y_test.to_numpy()\n",
    "\n",
    "# shuffle training data\n",
    "np.random.seed(42) # for reproducibility\n",
    "shuffle_index = np.random.permutation(len(X_train)) # generate random indices\n",
    "X_train, y_train = X_train[shuffle_index], y_train[shuffle_index] # shuffle data by applying reordering with the random indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part III: Scaling the data\n",
    "\n",
    "1. Standardize the training _and_ test data so that each feature has a mean of 0 and a standard deviation of 1.\n",
    "2. Check that the scaling was successful\n",
    "    - by printing the mean and standard deviation of each feature in the scaled training set\n",
    "    - by putting the scaled training set into a DataFrame and make a violin plot of the data\n",
    "\n",
    "__Hint:__ use the `axis` argument to calculate mean and standard deviation column-wise.\n",
    "\n",
    "__Important:__ Avoid data leakage!\n",
    "\n",
    "__More hints:__\n",
    "\n",
    "1. For each column, subtract the mean $(\\mu)$ of each column from each value in the column\n",
    "2. Divide the result by the standard deviation $(\\sigma)$ of the column\n",
    "\n",
    "(You saw how to do both operations in the lecture. If you don't remember, you can look it up in Canvas files.)\n",
    "\n",
    "Mathematically (in case this is useful for you), this transformation can be represented for each column as follows:\n",
    "\n",
    "$$ X_\\text{scaled} = \\frac{(X - \\mu)}{\\sigma} $$\n",
    "\n",
    "where:\n",
    "- $(X_\\text{scaled})$ are the new, transformed column values (a column-vector)\n",
    "- $(X)$ is the original values\n",
    "- $(\\mu)$ is the mean of the column\n",
    "- $(\\sigma)$ is the standard deviation of the column\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert your code below\n",
    "# ======================\n",
    "\n",
    "# 1\n",
    "X_train_scaled = (X_train - np.mean(X_train, axis=0)) / np.std(X_train, axis=0, ddof=1)\n",
    "X_test_scaled = (X_test - np.mean(X_train, axis=0)) / np.std(X_train, axis=0, ddof=1)\n",
    "\n",
    "# Double-check mean and std of the scaled X_train\n",
    "X_train_sc_mean = np.mean(X_train_scaled, axis=0)\n",
    "X_train_sc_std = np.std(X_train_scaled, axis=0, ddof=1)\n",
    "\n",
    "print(f'Mean of each feature in the scaled training set: \\n{X_train_sc_mean}')\n",
    "print()\n",
    "print(f'Standard deviation of each feature in the scaled training set: {X_train_sc_std}')\n",
    "\n",
    "df_scaled_train = pd.DataFrame(X_train_scaled)\n",
    "# Making a violin plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.title('Violin Plot of Scaled Training Data')\n",
    "sns.violinplot(data=df_scaled_train)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part IV: Training and evaluation with different dataset sizes and training times\n",
    "\n",
    "Often, a larger dataset size will yield better model performance. (As we will learn later, this usually prevents overfitting and increases the generalization capability of the trained model.)\n",
    "However, collecting data is usually rather expensive.\n",
    "\n",
    "In this part of the exercise, you will investigate\n",
    "\n",
    "- how the model performance changes with varying dataset size\n",
    "- how the model performance changes with varying numbers of epochs/iterations of the optimizer/solver (increasing training time).\n",
    "\n",
    "For this task (Part IV), use the `Adaline`, `Perceptron`, and `LogisticRegression` classifier from the `mlxtend` library. All use the gradient descent (GD) algorithm for training.\n",
    "\n",
    "__Important__: Use a learning rate of `1e-4` (`0.0001`) for all classifiers, and use the argument `minibatches=1` when initializing `Adaline` and `LogisticRegression` classifier (this will make sure it uses GD). For all three classifiers, pass `random_seed=42` when initializing the classifier to ensure reproducibility of the results.\n",
    "\n",
    "### Model training\n",
    "\n",
    "Train the model models using progressively larger subsets of your dataset, specifically: first 50 rows, first 100 rows, first 150 rows, ..., first 650 rows, first 700 rows (in total $14$ different variants).\n",
    "\n",
    "For each number of rows train the model with progressively larger number of epochs: 2, 7, 12, 17, ..., 87, 92, 97 (in total $20$ different model variants).\n",
    "\n",
    "The resulting $14 \\times 20 = 280$ models obtained from the different combinations of subsets and number of epochs. An output of the training process could look like this:\n",
    "\n",
    "```\n",
    "Model (1) Train a model with first 50 rows of data for 2 epochs\n",
    "Model (2) Train a model with first 50 rows of data for 7 epochs\n",
    "Model (3) Train a model with first 50 rows of data for 12 epochs\n",
    "...\n",
    "Model (21) Train a model with first 100 rows of data for 2 epochs\n",
    "Model (22) Train a model with first 100 rows of data for 7 epochs\n",
    "...\n",
    "Model (279) Train a model with first 700 rows of data for 92 epochs\n",
    "Model (280) Train a model with first 700 rows of data for 97 epochs\n",
    "```\n",
    "\n",
    "### Model evaluation\n",
    "\n",
    "For each of the $280$ models, calculate the __accuracy on the test set__ (do __not__ use the score method but compute accuracy yourself).\n",
    "Store the results in the provided 2D numpy array (it has $14$ rows and $20$ columns).\n",
    "The rows of the array correspond to the different dataset sizes, and the columns correspond to the different numbers of epochs.\n",
    "\n",
    "### Tasks\n",
    "1. Train the $280$ Adaline classifiers as mentioned above and calculate the accuracy for each of the $280$ variants.\n",
    "2. Generalize your code so that is doing the same procedure for all three classifiers: `Perceptron`, `Adaline`, and `LogisticRegression` after each other. Store the result for all classifiers. You can for example use an array of shape $3\\times14\\times20$ to store the accuracies of the three classifiers.\n",
    "\n",
    "Note that executing the cells will take some time (but on most systems it should not be more than 5 minutes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and evaluate all model variants\n",
    "# Insert your code below\n",
    "\n",
    "# Define parameters\n",
    "eta = 0.0001\n",
    "minibatches = 1\n",
    "random_seed = 42\n",
    "dataset_sizes = np.arange(50, 701, 50) # 14 rows\n",
    "num_epochs = np.arange(2, 98, 5) # 20 columns\n",
    "model_idx = 1\n",
    "\n",
    "# Initialize classifiers\n",
    "all_accuracies = np.zeros((3, len(dataset_sizes), len(num_epochs))) # Stores the accuracies\n",
    "\n",
    "# Loop over dataset sizes\n",
    "for size_idx, size in enumerate(dataset_sizes):\n",
    "    X_subset = X_train_scaled[:size]\n",
    "    y_subset = y_train[:size]\n",
    "    # Loop over number of epochs\n",
    "    for epoch_idx, epochs in enumerate(num_epochs):\n",
    "        perceptron = Perceptron(eta=eta, random_seed=random_seed, epochs = epochs)\n",
    "        perceptron.fit(X_subset, y_subset) # Train the classifier\n",
    "        y_pred = perceptron.predict(X_test_scaled)   # Make predictions on the test set\n",
    "        accuracy = np.mean(y_pred == y_test)  # Calculates the accuracy\n",
    "        all_accuracies[0][size_idx][epoch_idx] = accuracy  # Stores the accuracy in all_accuracies\n",
    "        \n",
    "        print(f\"Model ({model_idx}) - Percetron         - Train a model with first {size} rows of data for {epochs} epochs. Accuracy: {accuracy:.2f}\")\n",
    "        model_idx += 1\n",
    "        adaline = Adaline(eta=eta, minibatches=minibatches, random_seed=random_seed, epochs=epochs)\n",
    "        adaline.fit(X_subset, y_subset) # Train the classifier\n",
    "        y_pred = adaline.predict(X_test_scaled)   # Make predictions on the test set\n",
    "        accuracy = np.mean(y_pred == y_test)  # Calculates the accuracy\n",
    "        all_accuracies[1][size_idx][epoch_idx] = accuracy  # Stores the accuracy in all_accuracies\n",
    "      \n",
    "        print(f\"Model ({model_idx}) - Adaline           - Train a model with first {size} rows of data for {epochs} epochs. Accuracy: {accuracy:.2f}\")\n",
    "        model_idx += 1\n",
    "        LR = LogisticRegression(eta=eta, minibatches=minibatches, random_seed=random_seed, epochs = epochs)\n",
    "        LR.fit(X_subset, y_subset) # Train the classifier\n",
    "        y_pred = LR.predict(X_test_scaled)   # Make predictions on the test set\n",
    "        accuracy = np.mean(y_pred == y_test)  # Calculates the accuracy\n",
    "        all_accuracies[2][size_idx][epoch_idx] = accuracy  # Stores the accuracy in all_accuracies\n",
    "\n",
    "        print(f\"Model ({model_idx}) - Linear Regression - Train a model with first {size} rows of data for {epochs} epochs. Accuracy: {accuracy:.2f}\")\n",
    "        model_idx += 1 # update model ID\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance visualization\n",
    "\n",
    "Plot the performance measure for all classifiers (accuracy on the test set; use the result array from above) of all the $280$ variants for each classifier in a total of three heatmaps using, for example `seaborn` or `matplotlib` directly.\n",
    "\n",
    "The color should represent the accuracy on the test set, and the x and y axes should represent the number of epochs and the dataset size, respectively.\n",
    "Which one is x and which one is y is up to you to decide. Look in the example output at the top of the assignment for inspiration for how the plot could look like and how it could be labeled nicely. (But use the correct numbers corresponding to your dataset sizes and number of epochs.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# compute the absolute min and max for the accurancies\n",
    "min_accuracy = np.min([np.min(accuracy) for accuracy in all_accuracies])\n",
    "max_accuracy = np.max([np.max(accuracy) for accuracy in all_accuracies])\n",
    "\n",
    "classifiers = ['Perceptron', 'Adaline','Logistic Regression']\n",
    "# Plot the heatmaps for each classifier\n",
    "for idx in range(len(classifiers)):\n",
    "    classifier_name = classifiers[idx]\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.imshow(all_accuracies[idx], cmap='magma')# change it to the color from the example plot\n",
    "    plt.colorbar(label='Accuracy', ax=plt.gca())  # Make sure the color bar corresponds to the current axes\n",
    "    plt.clim(min_accuracy, max_accuracy)  # Set the range of the color bar\n",
    "    plt.title(f'{classifier_name} Accuracy Heatmap')\n",
    "    plt.xlabel('Number of Epochs')\n",
    "    plt.ylabel('Dataset size')\n",
    "    plt.xticks(np.arange(len(num_epochs)), num_epochs)\n",
    "    plt.yticks(np.arange(len(dataset_sizes)), dataset_sizes)\n",
    "    plt.show()\n",
    "\n",
    "# something is not right with the accuracies \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part V: Some more plotting\n",
    "\n",
    "For the following cell to execute you need to have the variable `X_test_scaled` with all samples of the test set and the variable `y_test` with the corresponding labels.\n",
    "Complete at least up until Part III. Executing the cell will plot something.\n",
    "\n",
    "1. Add code comments explaining what the lines are doing\n",
    "2. What is the purpose of the plot?\n",
    "    - vizualise the boundaries \n",
    "3. Describe all components of the subplot and then comment in general on the entire plot. What does it show? What does it not show?\n",
    "    - colored mesh: predicted probabilities belong to each class. darker blue belongs indicates to belong to class 1.\n",
    "    - blue triangles: reperesent class 0\n",
    "    - yellow circles: reperesent class 1\n",
    "    - lines: the boundary where the probabilty is 0.5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and a logistic regression model with 300 epochs and learning rate 0.0001\n",
    "clf = LogisticRegression(eta = 0.0001, epochs = 300, minibatches=1, random_seed=42) \n",
    "clf.fit(X_test_scaled, y_test) \n",
    "\n",
    "fig, axes = plt.subplots(8, 8, figsize=(30, 30)) # initalise for the subplots\n",
    "for i in range(0, 8):\n",
    "    for j in range(0, 8):\n",
    "        feature_1 = i\n",
    "        feature_2 = j\n",
    "        ax = axes[i, j] # \n",
    "\n",
    "        #set labels for x-axis and y-axis\n",
    "        ax.set_xlabel(f\"Feature {feature_1}\") \n",
    "\n",
    "        # calculate min and max value\n",
    "        mins = X_test_scaled.min(axis=0)\n",
    "        maxs = X_test_scaled.max(axis=0)\n",
    "\n",
    "        # makes the plot look nice by .....\n",
    "        x0 = np.linspace(mins[feature_1], maxs[feature_1], 100)\n",
    "        x1 = np.linspace(mins[feature_2], maxs[feature_2], 100)\n",
    "\n",
    "        # creates a meshgrid\n",
    "        X0, X1 = np.meshgrid(x0, x1) # makes a meshgrid\n",
    "        X_two_features = np.c_[X0.ravel(), X1.ravel()]\n",
    "        X_plot = np.zeros(shape=(X_two_features.shape[0], X_test_scaled.shape[1])) # makes the plot with zeroes\n",
    "\n",
    "        # give values to the plot\n",
    "        X_plot[:, feature_1] = X_two_features[:, 0] \n",
    "        X_plot[:, feature_2] = X_two_features[:, 1]\n",
    "\n",
    "        # predict the probability for a point in array\n",
    "        y_pred = clf.predict_proba(X_plot)\n",
    "        Z = y_pred.reshape(X0.shape)\n",
    "\n",
    "        # makes the plot\n",
    "        ax.pcolor(X0, X1, Z) # colored mesh\n",
    "        ax.contour(X0, X1, Z, levels=[0.5], colors='k') # countour lines where the probability is 0.5\n",
    "        ax.scatter(X_test_scaled[y_test == 0, feature_1], X_test_scaled[y_test == 0, feature_2], color=\"b\", marker=\"^\", s=50, facecolors=\"none\")\n",
    "        ax.scatter(X_test_scaled[y_test == 1, feature_1], X_test_scaled[y_test == 1, feature_2], color=\"y\", marker=\"o\", s=50, facecolors=\"none\")\n",
    "\n",
    "fig.tight_layout() # makes the layout look nice and that the subplots dont overlap\n",
    "plt.show() # display the plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part VI: Additional discussion\n",
    "\n",
    "### Part I:\n",
    "1. What kind of plots did you use to visualize the raw data, and why did you choose these types of plots?\n",
    "    - I used histograms, because it makes it easy to see if it is normal distributed and how the raw data is distributed.\n",
    "\n",
    "### Part II: \n",
    "1. What happens if we don't shuffle the training data before training the classifiers like in Part IV?\n",
    "    - if we don´t shuffle the training data, it could lead to biased training. \n",
    "    - All the examples from 0 and some from 1 would end up in training dataset and the test dataset would only be with 1. \n",
    "\n",
    "2. How could you do the same train/test split (Point 1.-4.) using scikit-learn?\n",
    "    - load the dataset with datasets.load\n",
    "    - using train_test_split to split the dataset into separat training and testing sets\n",
    "    - using standardScaler.fit from preprosessing to compjute the maean and sd for each festure and store this inside. the tranform to scale the other test and train, this to make sure data leakage doesnt happen.\n",
    "\n",
    "### Part IV:\n",
    "1. How does increasing the dataset size affect the performance of the logistic regression model? Provide a summary of your findings.\n",
    "    - increasing the datasize set usually leads to better preformance, because it has more information and gets more to learn from. \n",
    "    - You can se this by looking at the heatmap for logistic regression bottom line, the accuracy is the best here, and it increase as the epoch size increase.\n",
    "\n",
    "2. Describe the relationship between the number of epochs and model accuracy \n",
    "    - training with more epochs, gives the model more data to lean from, however it may lead to overfitting beacuse it memorizes the data. \n",
    "    - From the accurancy heatmaps you can se that for Adaline and Linear Regression, the accurancy increase with number of epochs. \n",
    "\n",
    "3. Which classifier is much slower to train and why do you think that is?\n",
    "    - from the accuracys it looks like its the Perceptron. \n",
    "    - This is beacuse the percetion updates the weights after evaulating each training sample, anf this takes a lot of time. \n",
    "    - Adaline calculateions is based on the wole trainingset, and therefor takes less time.\n",
    "\n",
    "4. One classifier shows strong fluctuations in accuracy for different dataset sizes and number of epochs. Which one is it and why do you think this happens?\n",
    "    - from the accuracy heatmaps it looks like its the Adaline classifier, this may be because of the Adaline rule(Widroff-Hoff rule). \n",
    "    - However, based on theory, it should have been the linear Regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
