{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA as PCAFromSklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principal component analysis\n",
    "\n",
    "Principal component analysis is an unsupervised learning technique. We only work on the feature matrix.\n",
    "\n",
    "\n",
    "We process the dataset $\\hat{\\boldsymbol{X}}$ given in form of a matrix with $n$ rows (samples) and $m$ columns (features).\n",
    "\n",
    "### 1. Centering\n",
    "\n",
    "We center the data by subtracting the mean of each feature from the corresponding column. We obtain the centered matrix $$\\boldsymbol{X} = \\hat{\\boldsymbol{X}} - \\boldsymbol{\\mu},$$ where $\\boldsymbol{\\mu}$ contains the mean of each column of $\\hat{\\boldsymbol{X}}$.\n",
    "\n",
    "### 2. Compute covariance matrix\n",
    "\n",
    "We compute the covariance matrix $$\\boldsymbol{\\Sigma} = \\frac{1}{n-1}\\boldsymbol{X}^T \\boldsymbol{X}.$$ The covariance matrix $\\boldsymbol{\\Sigma}$ is a $m \\times m$ matrix that is symmetric ($\\boldsymbol{\\Sigma} = \\boldsymbol{\\Sigma}^T$) and positive semi-definite ($\\boldsymbol{u}^T \\boldsymbol{\\Sigma} \\boldsymbol{u} \\leq 0 \\quad \\forall  \\boldsymbol{u} \\in \\mathbb{R}^m$) by construction. (The symbol $\\Sigma$ is not to be confused with the symbol for a sum. It is here simply the uppercase Greek letter sigma often used to denote the covariance matrix.)\n",
    "\n",
    "### 3. Eigendecomposition of the covariance matrix\n",
    "\n",
    "We compute the eigenvalues and eigenvectors of $\\boldsymbol{\\Sigma}$. This is also called the eigendecomposition of $\\boldsymbol{\\Sigma}$ because $\\boldsymbol{\\Sigma}$ can be written as $$\\boldsymbol{\\Sigma} = \\boldsymbol{W} \\boldsymbol{\\Lambda} \\boldsymbol{W}^T,$$ where $\\boldsymbol{\\Lambda}$ is a diagonal matrix containing the eigenvalues of $\\boldsymbol{\\Sigma}$ on the diagonal and $\\boldsymbol{W}$ is a matrix containing the eigenvectors of $\\boldsymbol{\\Sigma}$ as columns. Eigenvalues are the values that satisfy the following equation: $$\\boldsymbol{\\Sigma} \\boldsymbol{v} = \\lambda \\boldsymbol{v},$$ where $\\lambda$ is an eigenvalue and $\\boldsymbol{v}$ is the corresponding eigenvector. The eigenvectors are the directions of the new coordinate system and the eigenvalues are the variances of the data along these directions. The eigenvectors are orthogonal to each other, meaning that the inner product of any two eigenvectors is zero. For a positive semi-definite matrix, the eigenvalues are non-negative.\n",
    "\n",
    "### 4. Select the first $k$ principal components and build the transformation matrix\n",
    "\n",
    "We sort the eigenvalues in decreasing order and sort the corresponding eigenvectors accordingly.\n",
    "\n",
    "We then select the first $k$ eigenvectors to form the reduced matrix $\\boldsymbol{W}_k$ (the matrix $\\boldsymbol{W}_k$ has $m$ rows and $k$ colums). This matrix is used to transform the data into the new coordinate system. Here is how this works: Take a sample $\\boldsymbol{x}^{(i)}$ (a row of $\\boldsymbol{X}$) which is a row vector of the form $\\boldsymbol{x}^{(i)} = [\\boldsymbol{x}_1^{(i)}, \\boldsymbol{x}_2^{(i)}, \\ldots, \\boldsymbol{x}_m^{(i)}]$. The transformed feature vector is given by $$\\boldsymbol{z}^{(i)} = \\boldsymbol{\\phi}_\\text{PCA(k)}(\\boldsymbol{x}^{(i)}) = \\boldsymbol{x}^{(i)}\\boldsymbol{W}_k.$$\n",
    "It has the form $\\boldsymbol{z}^{(i)} = [z_1^{(i)}, z_2^{(i)}, \\ldots, z_k^{(i)}]$, so it has only $k$ features.\n",
    "We can do this for all samples and obtain the transformed feature matrix $$\\boldsymbol{X}_\\text{PCA(k)} := \\boldsymbol{Z} = \\boldsymbol{X} \\boldsymbol{W}_k.$$\n",
    "This matrix has $n$ rows and $k$ columns. So we have reduced the number of features from $m$ to $k$.\n",
    "\n",
    "### Remarks\n",
    "\n",
    "In practice computing the covariance matrix may be expensive and numerically unstable. There are more efficient ways to compute the principal components. The most common one is the singular value decomposition (SVD) of the centered matrix $\\boldsymbol{X}$. The SVD of $\\boldsymbol{X}$ is given by $$\\boldsymbol{X} = \\boldsymbol{U} \\boldsymbol{S} \\boldsymbol{V}^T,$$ where $\\boldsymbol{U}$ is a $n \\times m$ matrix, $\\boldsymbol{S}$ is a $m \\times m$ diagonal matrix and $\\boldsymbol{V}$ is a $n \\times m$ matrix. The columns of $\\boldsymbol{V}$ are the eigenvectors of $\\boldsymbol{X}^T \\boldsymbol{X}$ and the diagonal elements of $\\boldsymbol{S}$ are the square roots of the eigenvalues of $\\boldsymbol{X}^T \\boldsymbol{X}$. The columns of $\\boldsymbol{U}$ are the eigenvectors of $\\boldsymbol{X} \\boldsymbol{X}^T$. You might notice that this is therefore exactly the same as we have done above (apart from the factor $\\frac{1}{n-1}$ which can be easily added). However, there are efficient algorithms to compute the SVD of a matrix directly without forming $\\boldsymbol{X}^T \\boldsymbol{X}$. Moreover, if only the first $k$ principal components are needed, the SVD can be truncated to only keep the first $k$ columns of $\\boldsymbol{U}$ and $\\boldsymbol{S}$. There are algorithms that can compute the truncated SVD directly without computing the full SVD first saving time and memory.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation of PCA based on Eigendecomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PCA:\n",
    "    \"\"\"\n",
    "    Perform principal component analysis (PCA) on a dataset\n",
    "    \"\"\"\n",
    "    def __init__(self, n_components: int):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            n_components (int): Number of principal components to keep\n",
    "        \"\"\"\n",
    "        self.n_components = n_components\n",
    "\n",
    "    def fit(self, X: np.ndarray):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            X (np.ndarray): Data matrix of shape (n_samples, n_features)\n",
    "        \"\"\"\n",
    "        # First, we need to center the data\n",
    "        self.mean_ = X.mean(axis=0)\n",
    "        X_centered = X - self.mean_\n",
    "\n",
    "        # Then, we compute the covariance matrix from X as 1/(n-1)*X^T*X\n",
    "        self.covariance_ = np.dot(X_centered.T, X_centered) / (X_centered.shape[0] - 1)\n",
    "\n",
    "        # We compute the eigenvalue decomposition of the covariance matrix (Σ = WΛW^T)\n",
    "        # where each eigenvalue λ satifies the equation Σv = λv\n",
    "        # where v is a corresponding eigenvector.\n",
    "        # The function np.linalg.eig returns the eigenvalues as a vector\n",
    "        # and the eigenvectors as a matrix where each column corresponds to an eigenvector\n",
    "        self.eigenvalues_, self.eigenvectors_ = np.linalg.eigh(self.covariance_)\n",
    "\n",
    "        # Sort the eigenvectors by decreasing eigenvalues\n",
    "        # (or get the index permutation to sort eigenvalues in descending order\n",
    "        # and apply this permutation to the eigenvalues vector and eigenvectors matrix)\n",
    "        idx = self.eigenvalues_.argsort()[::-1]\n",
    "        self.eigenvalues_ = self.eigenvalues_[idx]\n",
    "        self.eigenvectors_ = self.eigenvectors_[:, idx]\n",
    "\n",
    "        # Keep only the first n_components eigenvectors (the user asked for in the constructor)\n",
    "        self.components_ = self.eigenvectors_[:, :self.n_components]\n",
    "\n",
    "    def transform(self, X: np.ndarray):\n",
    "        \"\"\"\n",
    "        Apply the transformation resulting in a dimensionality reduction of X\n",
    "        to the first n_components principal components. In other words, project\n",
    "        the data onto the principal components. In formulas, this is XW where\n",
    "        X is the data matrix and W is the matrix of principal components (eigenvectors of the covariance matrix).\n",
    "\n",
    "        Args:\n",
    "            X (np.ndarray): Data matrix of shape (n_samples, n_features)\n",
    "        \"\"\"\n",
    "        return np.dot(X - self.mean_, self.components_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example on a 2D dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_2d_example_pca(PCA):\n",
    "    # Create a 2D dataset multi-dimensional Gaussian distribution\n",
    "    np.random.seed(0)\n",
    "    X = np.dot([[1.0, 0.4],[0.5, 1]], np.random.randn(2, 500)).T\n",
    "\n",
    "    # Fit PCA\n",
    "    pca = PCA(n_components=2)\n",
    "    pca.fit(X)\n",
    "\n",
    "    # Plot the data\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "    # Data with principal components as arrows\n",
    "    ax[0].scatter(X[:, 0], X[:, 1])\n",
    "    ax[0].arrow(0, 0, pca.components_[0, 0], pca.components_[0, 1], head_width=0.2, head_length=0.2, fc='k', ec='k')\n",
    "    ax[0].arrow(0, 0, pca.components_[1, 0], pca.components_[1, 1], head_width=0.2, head_length=0.2, fc='k', ec='k')\n",
    "    ax[0].text(pca.components_[0, 0] + 0.5, pca.components_[0, 1], 'PC1', ha='center', va='center', fontsize=12, color='black', fontweight='bold')\n",
    "    ax[0].text(pca.components_[1, 0] + 0.5, pca.components_[1, 1], 'PC2', ha='center', va='center', fontsize=12, color='black', fontweight='bold')\n",
    "    ax[0].set_xlabel('Feature 1')\n",
    "    ax[0].set_ylabel('Feature 2')\n",
    "\n",
    "    # Transform data to principal directions and plot again\n",
    "    X_pca = pca.transform(X)\n",
    "    ax[1].scatter(X_pca[:, 0], X_pca[:, 1])\n",
    "    ax[1].set_xlabel('Principle component 1 (PC1)')\n",
    "    ax[1].set_ylabel('Principle component 2 (PC2)')\n",
    "\n",
    "    # Perform another PCA on the transformed data to show that\n",
    "    # the principal components are orthogonal to the axis now\n",
    "    pca = PCA(n_components=2)\n",
    "    pca.fit(X_pca)\n",
    "\n",
    "    ax[1].arrow(0, 0, pca.components_[0, 0], pca.components_[0, 1], head_width=0.2, head_length=0.2, fc='k', ec='k')\n",
    "    ax[1].arrow(0, 0, pca.components_[1, 0], pca.components_[1, 1], head_width=0.2, head_length=0.2, fc='k', ec='k')\n",
    "    ax[1].text(pca.components_[0, 0] + 0.5, pca.components_[0, 1], 'PC1', ha='center', va='center', fontsize=12, color='black', fontweight='bold')\n",
    "    ax[1].text(pca.components_[1, 0] + 0.5, pca.components_[1, 1], 'PC2', ha='center', va='center', fontsize=12, color='black', fontweight='bold')\n",
    "\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot PCA using the PCA class implemented above\n",
    "plot_2d_example_pca(PCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot PCA using the PCA class from sklearn\n",
    "plot_2d_example_pca(PCAFromSklearn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remark\n",
    "\n",
    "In comparison with the `sklearn` implementation, the implemented PCA class\n",
    "here might return the principal components with the opposite sign. This is\n",
    "not an issue as the principal components are only defined up to a sign\n",
    "and the sign can be flipped without changing the meaning of the principal\n",
    "components. If $\\boldsymbol{\\Sigma}$ is the covariance matrix and $\\boldsymbol{v}$ is an eigenvector of $\\boldsymbol{\\Sigma}$,\n",
    "and $\\lambda$ is the corresponding eigenvalue, then $\\boldsymbol{\\Sigma}\\boldsymbol{v} = \\lambda \\boldsymbol{v}$ and also $\\boldsymbol{\\Sigma}(-\\boldsymbol{v}) = \\lambda (-\\boldsymbol{v})$.\n",
    "This means the eigenvalue equation is satisfied for both $\\boldsymbol{v}$ and $-\\boldsymbol{v}$. (We can\n",
    "also multiply $\\boldsymbol{v}$ by any non-zero scalar and the equation is still satisfied.\n",
    "Usually eigenvectors are normalized to have a length of $1$ by convention.)\n",
    "\n",
    "This simply has the effect that the plot is mirrored at the origin."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
